{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilhermemendoncafreire/mambaforge/envs/ml/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.8.0 and strictly below 2.11.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.11.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib as mplt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 180)\n",
      "(21, 180)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('output/masked_10_missing_180x21.csv', sep=';', header=None)\n",
    "Q = pd.read_csv('data/Qmatrix.csv', sep=';', header=None)\n",
    "Q = Q.T\n",
    "print(data.shape)\n",
    "print(Q.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict connection in decoder\n",
    "def q_constraint(w):\n",
    "    target = w * Q\n",
    "    diff = w - target\n",
    "    w = w * tf.cast(tf.math.equal(diff, 0), keras.backend.floatx()) \n",
    "    return w * tf.cast(tf.math.greater_equal(w, 0), keras.backend.floatx())\n",
    "\n",
    "# Remove zeros function\n",
    "def remove_zeros(arr):\n",
    "  n_arr = []\n",
    "  \n",
    "  for j in range(NUM_SKILLS): \n",
    "    for i in range(NUM_STATS):\n",
    "      if Q.iloc[j, i] != 0:\n",
    "        n_arr.append(arr[j][i])\n",
    "  \n",
    "  return n_arr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stats and skills\n",
    "NUM_STATS = 180\n",
    "NUM_SKILLS = 21\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "BUFFER_SIZE = 1024\n",
    "\n",
    "INTERMEDIATE_DIM = 40\n",
    "N_DECODERS = 1\n",
    "\n",
    "# Encoder and Decoder\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "ENC_PROJECTION_DIM = 1\n",
    "DEC_PROJECTION_DIM = 18\n",
    "ENC_NUM_HEADS = 4\n",
    "ENC_LAYERS = 6\n",
    "DEC_NUM_HEADS = 4\n",
    "DEC_LAYERS = (\n",
    "    2  # The decoder is lightweight but should be reasonably deep for reconstruction.\n",
    ")\n",
    "ENC_TRANSFORMER_UNITS = [\n",
    "    ENC_PROJECTION_DIM * 2,\n",
    "    ENC_PROJECTION_DIM,\n",
    "]  # Size of the transformer layers.\n",
    "DEC_TRANSFORMER_UNITS = [\n",
    "    DEC_PROJECTION_DIM * 2,\n",
    "    DEC_PROJECTION_DIM,\n",
    "]\n",
    "\n",
    "# Optimizer\n",
    "LEARNING_RATE = 5e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Number of persons\n",
    "NUM_PERSONS = data.shape[0]\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 5e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "MASK_PROPORTION = 0.10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "  \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    z_mean, z_log_var = inputs\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class Encoder(keras.Model):\n",
    "    \"\"\"Maps items respone to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        latent_dim, \n",
    "        intermediate_dim,\n",
    "        n_decoders, \n",
    "        name=\"encoder\", \n",
    "        **kwargs\n",
    "    ):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.n_decoders = n_decoders\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation='tanh')\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "\n",
    "        if self.n_decoders > 1:\n",
    "          return z_mean, z_log_var, [self.sampling((z_mean, z_log_var)) for i in range(self.n_decoders)]\n",
    "      \n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "class Decoder(keras.Model):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, latent_dim, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\", kernel_constraint=q_constraint)#,kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)),bias_regularizer=regularizers.l2(1e-4),activity_regularizer=regularizers.l2(1e-5))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense_output(inputs)\n",
    "\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim,\n",
    "        intermediate_dim,\n",
    "        num_skills,\n",
    "        n_decoders,\n",
    "        name=\"autoencoder\"\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name)\n",
    "        #self.original_dim = original_dim\n",
    "        self.n_decoders = n_decoders\n",
    "        self.encoder = Encoder(latent_dim=num_skills, intermediate_dim=intermediate_dim, n_decoders=n_decoders)\n",
    "        self.decoder = Decoder(original_dim, latent_dim=num_skills)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.z_mean, self.z_log_var, self.z = self.encoder(inputs)\n",
    "        \n",
    "        if self.n_decoders > 1:\n",
    "          return [self.decoder(self.z[i]) for i in range(self.n_decoders)]\n",
    "\n",
    "        reconstructed = self.decoder(self.z)\n",
    "        \n",
    "        return reconstructed\n",
    "\n",
    "    # Loss function\n",
    "    def vae_loss(self, input, output):\n",
    "        cross_entropy_loss = (NUM_STATS/ 1.0) * (tf.reduce_mean((0.5 * tf.math.square(input) + 0.5 * input) * (-1) * tf.math.log(output) + \n",
    "        (1 - tf.math.square(input)) * (-1) * tf.math.log(1 - output)))  \n",
    "        kl_loss = -0.5 * tf.reduce_mean(self.z_log_var - tf.square(self.z_mean) - tf.exp(self.z_log_var) + 1, axis=-1)\n",
    "        return cross_entropy_loss + kl_loss\n",
    "\n",
    "    # Get weights\n",
    "    def _get_weights(self):\n",
    "        return self.decoder.trainable_weights\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 Iteration #################################################################################### \n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 07:44:38.808315: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 1s 881us/step - loss: 109.5240 - binary_accuracy: 0.5535\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 0s 698us/step - loss: 105.2452 - binary_accuracy: 0.5770\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 0s 686us/step - loss: 102.8344 - binary_accuracy: 0.5847\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 0s 696us/step - loss: 99.9792 - binary_accuracy: 0.6021\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 0s 688us/step - loss: 96.4292 - binary_accuracy: 0.6206\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 0s 688us/step - loss: 93.6542 - binary_accuracy: 0.6340\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 0s 660us/step - loss: 91.3561 - binary_accuracy: 0.6440\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 0s 639us/step - loss: 89.6929 - binary_accuracy: 0.6514\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 0s 633us/step - loss: 88.2966 - binary_accuracy: 0.6571\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 0s 766us/step - loss: 87.1066 - binary_accuracy: 0.6620\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 0s 652us/step - loss: 86.0336 - binary_accuracy: 0.6670\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 0s 655us/step - loss: 85.0413 - binary_accuracy: 0.6709\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 0s 652us/step - loss: 84.2543 - binary_accuracy: 0.6741\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 0s 645us/step - loss: 83.6893 - binary_accuracy: 0.6766\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 0s 644us/step - loss: 83.2942 - binary_accuracy: 0.6778\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 0s 661us/step - loss: 82.9395 - binary_accuracy: 0.6793\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 0s 634us/step - loss: 82.6355 - binary_accuracy: 0.6810\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 0s 628us/step - loss: 82.3701 - binary_accuracy: 0.6821\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 0s 795us/step - loss: 82.0799 - binary_accuracy: 0.6833\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 0s 630us/step - loss: 81.8662 - binary_accuracy: 0.6843\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 0s 632us/step - loss: 81.6294 - binary_accuracy: 0.6850\n",
      "Epoch 22/100\n",
      "160/160 [==============================] - 0s 663us/step - loss: 81.5540 - binary_accuracy: 0.6855\n",
      "Epoch 23/100\n",
      "160/160 [==============================] - 0s 642us/step - loss: 81.4387 - binary_accuracy: 0.6859\n",
      "Epoch 24/100\n",
      "160/160 [==============================] - 0s 631us/step - loss: 81.3794 - binary_accuracy: 0.6858\n",
      "Epoch 25/100\n",
      "160/160 [==============================] - 0s 622us/step - loss: 81.2882 - binary_accuracy: 0.6863\n",
      "Epoch 26/100\n",
      "160/160 [==============================] - 0s 716us/step - loss: 81.1964 - binary_accuracy: 0.6867\n",
      "Epoch 27/100\n",
      "160/160 [==============================] - 0s 651us/step - loss: 81.1348 - binary_accuracy: 0.6868\n",
      "Epoch 28/100\n",
      "160/160 [==============================] - 0s 852us/step - loss: 81.1451 - binary_accuracy: 0.6868\n",
      "Epoch 29/100\n",
      "160/160 [==============================] - 0s 677us/step - loss: 81.0966 - binary_accuracy: 0.6869\n",
      "Epoch 30/100\n",
      "160/160 [==============================] - 0s 675us/step - loss: 81.0550 - binary_accuracy: 0.6872\n",
      "Epoch 31/100\n",
      "160/160 [==============================] - 0s 667us/step - loss: 81.0233 - binary_accuracy: 0.6870\n",
      "Epoch 32/100\n",
      "160/160 [==============================] - 0s 687us/step - loss: 80.9955 - binary_accuracy: 0.6871\n",
      "Epoch 33/100\n",
      "160/160 [==============================] - 0s 667us/step - loss: 80.9950 - binary_accuracy: 0.6873\n",
      "Epoch 34/100\n",
      "160/160 [==============================] - 0s 653us/step - loss: 80.8975 - binary_accuracy: 0.6875\n",
      "Epoch 35/100\n",
      "160/160 [==============================] - 0s 668us/step - loss: 80.9034 - binary_accuracy: 0.6874\n",
      "Epoch 36/100\n",
      "160/160 [==============================] - 0s 677us/step - loss: 80.8811 - binary_accuracy: 0.6875\n",
      "Epoch 37/100\n",
      "160/160 [==============================] - 0s 836us/step - loss: 80.8794 - binary_accuracy: 0.6874\n",
      "Epoch 38/100\n",
      "160/160 [==============================] - 0s 670us/step - loss: 80.8577 - binary_accuracy: 0.6875\n",
      "Epoch 39/100\n",
      "160/160 [==============================] - 0s 674us/step - loss: 80.9159 - binary_accuracy: 0.6875\n",
      "Epoch 40/100\n",
      "160/160 [==============================] - 0s 663us/step - loss: 80.8523 - binary_accuracy: 0.6876\n",
      "Epoch 41/100\n",
      "160/160 [==============================] - 0s 677us/step - loss: 80.8198 - binary_accuracy: 0.6875\n",
      "Epoch 42/100\n",
      "160/160 [==============================] - 0s 663us/step - loss: 80.8106 - binary_accuracy: 0.6876\n",
      "Epoch 43/100\n",
      "160/160 [==============================] - 0s 672us/step - loss: 80.8134 - binary_accuracy: 0.6876\n",
      "Epoch 44/100\n",
      "160/160 [==============================] - 0s 669us/step - loss: 80.7883 - binary_accuracy: 0.6875\n",
      "Epoch 45/100\n",
      "160/160 [==============================] - 0s 675us/step - loss: 80.7987 - binary_accuracy: 0.6876\n",
      "Epoch 46/100\n",
      "160/160 [==============================] - 0s 847us/step - loss: 80.7791 - binary_accuracy: 0.6874\n",
      "Epoch 47/100\n",
      "160/160 [==============================] - 0s 684us/step - loss: 80.7460 - binary_accuracy: 0.6875\n",
      "Epoch 48/100\n",
      "160/160 [==============================] - 0s 784us/step - loss: 80.7809 - binary_accuracy: 0.6877\n",
      "Epoch 49/100\n",
      "160/160 [==============================] - 0s 671us/step - loss: 80.7554 - binary_accuracy: 0.6877\n",
      "Epoch 50/100\n",
      "160/160 [==============================] - 0s 695us/step - loss: 80.7183 - binary_accuracy: 0.6877\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 0s 710us/step - loss: 80.7412 - binary_accuracy: 0.6877\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 0s 708us/step - loss: 80.6654 - binary_accuracy: 0.6882\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 0s 742us/step - loss: 80.7002 - binary_accuracy: 0.6879\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 0s 785us/step - loss: 80.6932 - binary_accuracy: 0.6878\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 0s 674us/step - loss: 80.7162 - binary_accuracy: 0.6878\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 0s 692us/step - loss: 80.6831 - binary_accuracy: 0.6880\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 0s 672us/step - loss: 80.6477 - binary_accuracy: 0.6880\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - 0s 634us/step - loss: 80.6226 - binary_accuracy: 0.6880\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 0s 640us/step - loss: 80.6833 - binary_accuracy: 0.6879\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 0s 626us/step - loss: 80.6495 - binary_accuracy: 0.6881\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 0s 621us/step - loss: 80.6368 - binary_accuracy: 0.6881\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 0s 639us/step - loss: 80.6382 - binary_accuracy: 0.6882\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 0s 771us/step - loss: 80.6120 - binary_accuracy: 0.6882\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 0s 640us/step - loss: 80.6307 - binary_accuracy: 0.6882\n",
      "Epoch 65/100\n",
      "160/160 [==============================] - 0s 638us/step - loss: 80.5975 - binary_accuracy: 0.6882\n",
      "Epoch 66/100\n",
      "160/160 [==============================] - 0s 639us/step - loss: 80.5939 - binary_accuracy: 0.6879\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 0s 647us/step - loss: 80.6172 - binary_accuracy: 0.6880\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 0s 645us/step - loss: 80.6030 - binary_accuracy: 0.6883\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 0s 647us/step - loss: 80.5814 - binary_accuracy: 0.6883\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 0s 642us/step - loss: 80.5846 - binary_accuracy: 0.6880\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 0s 753us/step - loss: 80.5753 - binary_accuracy: 0.6883\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 0s 642us/step - loss: 80.5635 - binary_accuracy: 0.6883\n",
      "Epoch 73/100\n",
      "160/160 [==============================] - 0s 630us/step - loss: 80.5654 - binary_accuracy: 0.6882\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 0s 742us/step - loss: 80.5497 - binary_accuracy: 0.6880\n",
      "Epoch 75/100\n",
      "160/160 [==============================] - 0s 630us/step - loss: 80.5120 - binary_accuracy: 0.6883\n",
      "Epoch 76/100\n",
      "160/160 [==============================] - 0s 633us/step - loss: 80.5075 - binary_accuracy: 0.6883\n",
      "Epoch 77/100\n",
      "160/160 [==============================] - 0s 623us/step - loss: 80.5199 - binary_accuracy: 0.6882\n",
      "Epoch 78/100\n",
      "160/160 [==============================] - 0s 633us/step - loss: 80.4904 - binary_accuracy: 0.6885\n",
      "Epoch 79/100\n",
      "160/160 [==============================] - 0s 652us/step - loss: 80.4954 - binary_accuracy: 0.6882\n",
      "Epoch 80/100\n",
      "160/160 [==============================] - 0s 647us/step - loss: 80.4876 - binary_accuracy: 0.6885\n",
      "Epoch 81/100\n",
      "160/160 [==============================] - 0s 789us/step - loss: 80.4532 - binary_accuracy: 0.6883\n",
      "Epoch 82/100\n",
      "160/160 [==============================] - 0s 633us/step - loss: 80.4684 - binary_accuracy: 0.6883\n",
      "Epoch 83/100\n",
      "160/160 [==============================] - 0s 632us/step - loss: 80.4107 - binary_accuracy: 0.6885\n",
      "Epoch 84/100\n",
      "160/160 [==============================] - 0s 624us/step - loss: 80.4148 - binary_accuracy: 0.6887\n",
      "Epoch 85/100\n",
      "160/160 [==============================] - 0s 643us/step - loss: 80.3932 - binary_accuracy: 0.6886\n",
      "Epoch 86/100\n",
      "160/160 [==============================] - 0s 620us/step - loss: 80.3796 - binary_accuracy: 0.6888\n",
      "Epoch 87/100\n",
      "160/160 [==============================] - 0s 624us/step - loss: 80.3456 - binary_accuracy: 0.6888\n",
      "Epoch 88/100\n",
      "160/160 [==============================] - 0s 629us/step - loss: 80.3354 - binary_accuracy: 0.6887\n",
      "Epoch 89/100\n",
      "160/160 [==============================] - 0s 653us/step - loss: 80.3087 - binary_accuracy: 0.6889\n",
      "Epoch 90/100\n",
      "160/160 [==============================] - 0s 778us/step - loss: 80.2936 - binary_accuracy: 0.6889\n",
      "Epoch 91/100\n",
      "160/160 [==============================] - 0s 628us/step - loss: 80.2995 - binary_accuracy: 0.6890\n",
      "Epoch 92/100\n",
      "160/160 [==============================] - 0s 648us/step - loss: 80.2371 - binary_accuracy: 0.6891\n",
      "Epoch 93/100\n",
      "160/160 [==============================] - 0s 653us/step - loss: 80.2303 - binary_accuracy: 0.6892\n",
      "Epoch 94/100\n",
      "160/160 [==============================] - 0s 651us/step - loss: 80.2423 - binary_accuracy: 0.6892\n",
      "Epoch 95/100\n",
      "160/160 [==============================] - 0s 647us/step - loss: 80.2212 - binary_accuracy: 0.6890\n",
      "Epoch 96/100\n",
      "160/160 [==============================] - 0s 631us/step - loss: 80.2123 - binary_accuracy: 0.6891\n",
      "Epoch 97/100\n",
      "160/160 [==============================] - 0s 634us/step - loss: 80.1683 - binary_accuracy: 0.6894\n",
      "Epoch 98/100\n",
      "160/160 [==============================] - 0s 629us/step - loss: 80.1451 - binary_accuracy: 0.6894\n",
      "Epoch 99/100\n",
      "160/160 [==============================] - 0s 648us/step - loss: 80.1265 - binary_accuracy: 0.6895\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 0s 756us/step - loss: 80.1440 - binary_accuracy: 0.6894\n",
      "250/250 [==============================] - 0s 798us/step\n",
      "250/250 [==============================] - 0s 358us/step\n",
      "\n",
      " 2 Iteration #################################################################################### \n",
      "\n",
      "Epoch 1/100\n",
      "160/160 [==============================] - 0s 745us/step - loss: 109.4328 - binary_accuracy: 0.5552\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 0s 670us/step - loss: 104.3654 - binary_accuracy: 0.5852\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 0s 649us/step - loss: 100.8915 - binary_accuracy: 0.5974\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 0s 654us/step - loss: 98.1295 - binary_accuracy: 0.6129\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 0s 659us/step - loss: 95.0627 - binary_accuracy: 0.6284\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 0s 692us/step - loss: 92.7249 - binary_accuracy: 0.6386\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 0s 781us/step - loss: 91.1362 - binary_accuracy: 0.6453\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 0s 641us/step - loss: 89.8129 - binary_accuracy: 0.6510\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 0s 658us/step - loss: 88.5493 - binary_accuracy: 0.6571\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 0s 661us/step - loss: 87.3300 - binary_accuracy: 0.6623\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 0s 653us/step - loss: 86.2599 - binary_accuracy: 0.6668\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 0s 655us/step - loss: 85.2661 - binary_accuracy: 0.6710\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 0s 649us/step - loss: 84.4374 - binary_accuracy: 0.6740\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 0s 651us/step - loss: 83.8177 - binary_accuracy: 0.6764\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 0s 793us/step - loss: 83.3305 - binary_accuracy: 0.6787\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 0s 680us/step - loss: 82.9776 - binary_accuracy: 0.6800\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 0s 659us/step - loss: 82.7608 - binary_accuracy: 0.6808\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 0s 646us/step - loss: 82.5175 - binary_accuracy: 0.6818\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 0s 729us/step - loss: 82.4177 - binary_accuracy: 0.6820\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 0s 645us/step - loss: 82.2952 - binary_accuracy: 0.6824\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 0s 653us/step - loss: 82.1544 - binary_accuracy: 0.6829\n",
      "Epoch 22/100\n",
      "160/160 [==============================] - 0s 768us/step - loss: 82.0890 - binary_accuracy: 0.6832\n",
      "Epoch 23/100\n",
      "160/160 [==============================] - 0s 645us/step - loss: 81.9713 - binary_accuracy: 0.6835\n",
      "Epoch 24/100\n",
      "160/160 [==============================] - 0s 641us/step - loss: 81.9543 - binary_accuracy: 0.6831\n",
      "Epoch 25/100\n",
      "160/160 [==============================] - 0s 655us/step - loss: 81.9084 - binary_accuracy: 0.6833\n",
      "Epoch 26/100\n",
      "160/160 [==============================] - 0s 643us/step - loss: 81.7866 - binary_accuracy: 0.6839\n",
      "Epoch 27/100\n",
      "160/160 [==============================] - 0s 649us/step - loss: 81.8111 - binary_accuracy: 0.6837\n",
      "Epoch 28/100\n",
      "160/160 [==============================] - 0s 641us/step - loss: 81.7779 - binary_accuracy: 0.6839\n",
      "Epoch 29/100\n",
      "160/160 [==============================] - 0s 649us/step - loss: 81.7317 - binary_accuracy: 0.6840\n",
      "Epoch 30/100\n",
      "160/160 [==============================] - 0s 786us/step - loss: 81.7181 - binary_accuracy: 0.6840\n",
      "Epoch 31/100\n",
      "160/160 [==============================] - 0s 640us/step - loss: 81.6696 - binary_accuracy: 0.6841\n",
      "Epoch 32/100\n",
      "160/160 [==============================] - 0s 646us/step - loss: 81.5535 - binary_accuracy: 0.6844\n",
      "Epoch 33/100\n",
      "160/160 [==============================] - 0s 653us/step - loss: 81.4867 - binary_accuracy: 0.6851\n",
      "Epoch 34/100\n",
      "160/160 [==============================] - 0s 646us/step - loss: 81.2762 - binary_accuracy: 0.6860\n",
      "Epoch 35/100\n",
      "160/160 [==============================] - 0s 659us/step - loss: 81.0985 - binary_accuracy: 0.6866\n",
      "Epoch 36/100\n",
      "160/160 [==============================] - 0s 650us/step - loss: 80.9944 - binary_accuracy: 0.6871\n",
      "Epoch 37/100\n",
      "160/160 [==============================] - 0s 726us/step - loss: 80.9324 - binary_accuracy: 0.6872\n",
      "Epoch 38/100\n",
      "160/160 [==============================] - 0s 786us/step - loss: 80.8977 - binary_accuracy: 0.6872\n",
      "Epoch 39/100\n",
      "160/160 [==============================] - 0s 632us/step - loss: 80.8686 - binary_accuracy: 0.6874\n",
      "Epoch 40/100\n",
      "160/160 [==============================] - 0s 644us/step - loss: 80.8060 - binary_accuracy: 0.6876\n",
      "Epoch 41/100\n",
      "160/160 [==============================] - 0s 644us/step - loss: 80.8300 - binary_accuracy: 0.6876\n",
      "Epoch 42/100\n",
      "160/160 [==============================] - 0s 649us/step - loss: 80.7861 - binary_accuracy: 0.6878\n",
      "Epoch 43/100\n",
      "160/160 [==============================] - 0s 658us/step - loss: 80.7514 - binary_accuracy: 0.6881\n",
      "Epoch 44/100\n",
      "160/160 [==============================] - 0s 643us/step - loss: 80.7278 - binary_accuracy: 0.6879\n",
      "Epoch 45/100\n",
      "160/160 [==============================] - 0s 639us/step - loss: 80.7294 - binary_accuracy: 0.6878\n",
      "Epoch 46/100\n",
      "160/160 [==============================] - 0s 789us/step - loss: 80.7498 - binary_accuracy: 0.6875\n",
      "Epoch 47/100\n",
      "160/160 [==============================] - 0s 641us/step - loss: 80.6877 - binary_accuracy: 0.6879\n",
      "Epoch 48/100\n",
      "160/160 [==============================] - 0s 651us/step - loss: 80.6881 - binary_accuracy: 0.6879\n",
      "Epoch 49/100\n",
      "160/160 [==============================] - 0s 652us/step - loss: 80.6522 - binary_accuracy: 0.6881\n",
      "Epoch 50/100\n",
      "160/160 [==============================] - 0s 655us/step - loss: 80.6528 - binary_accuracy: 0.6880\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 0s 664us/step - loss: 80.6359 - binary_accuracy: 0.6883\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 0s 727us/step - loss: 80.6449 - binary_accuracy: 0.6880\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 0s 654us/step - loss: 80.6311 - binary_accuracy: 0.6881\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 0s 790us/step - loss: 80.5799 - binary_accuracy: 0.6882\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 0s 648us/step - loss: 80.5826 - binary_accuracy: 0.6882\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 0s 645us/step - loss: 80.5792 - binary_accuracy: 0.6882\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 0s 652us/step - loss: 80.5447 - binary_accuracy: 0.6884\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - 0s 655us/step - loss: 80.5996 - binary_accuracy: 0.6883\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 0s 653us/step - loss: 80.5286 - binary_accuracy: 0.6885\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 0s 668us/step - loss: 80.5303 - binary_accuracy: 0.6885\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 0s 643us/step - loss: 80.5046 - binary_accuracy: 0.6885\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 0s 645us/step - loss: 80.5696 - binary_accuracy: 0.6881\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 0s 776us/step - loss: 80.5069 - binary_accuracy: 0.6884\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 0s 650us/step - loss: 80.5195 - binary_accuracy: 0.6884\n",
      "Epoch 65/100\n",
      "160/160 [==============================] - 0s 650us/step - loss: 80.5223 - binary_accuracy: 0.6883\n",
      "Epoch 66/100\n",
      "160/160 [==============================] - 0s 650us/step - loss: 80.5082 - binary_accuracy: 0.6882\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 0s 648us/step - loss: 80.4935 - binary_accuracy: 0.6883\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 0s 659us/step - loss: 80.4769 - binary_accuracy: 0.6883\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 0s 643us/step - loss: 80.4706 - binary_accuracy: 0.6885\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 0s 743us/step - loss: 80.4510 - binary_accuracy: 0.6883\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 0s 785us/step - loss: 80.4397 - binary_accuracy: 0.6886\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 0s 634us/step - loss: 80.4579 - binary_accuracy: 0.6882\n",
      "Epoch 73/100\n",
      "160/160 [==============================] - 0s 642us/step - loss: 80.4731 - binary_accuracy: 0.6884\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 0s 644us/step - loss: 80.4570 - binary_accuracy: 0.6882\n",
      "Epoch 75/100\n",
      "160/160 [==============================] - 0s 630us/step - loss: 80.4519 - binary_accuracy: 0.6885\n",
      "Epoch 76/100\n",
      "160/160 [==============================] - 0s 629us/step - loss: 80.4374 - binary_accuracy: 0.6885\n",
      "Epoch 77/100\n",
      "160/160 [==============================] - 0s 657us/step - loss: 80.4158 - binary_accuracy: 0.6887\n",
      "Epoch 78/100\n",
      "160/160 [==============================] - 0s 645us/step - loss: 80.4208 - binary_accuracy: 0.6885\n",
      "Epoch 79/100\n",
      "160/160 [==============================] - 0s 648us/step - loss: 80.3910 - binary_accuracy: 0.6886\n",
      "Epoch 80/100\n",
      "160/160 [==============================] - 0s 793us/step - loss: 80.4195 - binary_accuracy: 0.6885\n",
      "Epoch 81/100\n",
      "160/160 [==============================] - 0s 646us/step - loss: 80.4005 - binary_accuracy: 0.6886\n",
      "Epoch 82/100\n",
      "160/160 [==============================] - 0s 642us/step - loss: 80.3824 - binary_accuracy: 0.6885\n",
      "Epoch 83/100\n",
      "160/160 [==============================] - 0s 657us/step - loss: 80.4018 - binary_accuracy: 0.6886\n",
      "Epoch 84/100\n",
      "160/160 [==============================] - 0s 649us/step - loss: 80.3614 - binary_accuracy: 0.6886\n",
      "Epoch 85/100\n",
      "160/160 [==============================] - 0s 651us/step - loss: 80.3571 - binary_accuracy: 0.6888\n",
      "Epoch 86/100\n",
      "160/160 [==============================] - 0s 647us/step - loss: 80.3896 - binary_accuracy: 0.6884\n",
      "Epoch 87/100\n",
      "160/160 [==============================] - 0s 738us/step - loss: 80.3880 - binary_accuracy: 0.6888\n",
      "Epoch 88/100\n",
      "160/160 [==============================] - 0s 763us/step - loss: 80.3528 - binary_accuracy: 0.6887\n",
      "Epoch 89/100\n",
      "160/160 [==============================] - 0s 659us/step - loss: 80.3173 - binary_accuracy: 0.6887\n",
      "Epoch 90/100\n",
      "160/160 [==============================] - 0s 647us/step - loss: 80.3409 - binary_accuracy: 0.6888\n",
      "Epoch 91/100\n",
      "160/160 [==============================] - 0s 642us/step - loss: 80.3295 - binary_accuracy: 0.6888\n",
      "Epoch 92/100\n",
      "160/160 [==============================] - 0s 655us/step - loss: 80.3345 - binary_accuracy: 0.6887\n",
      "Epoch 93/100\n",
      "160/160 [==============================] - 0s 643us/step - loss: 80.3332 - binary_accuracy: 0.6886\n",
      "Epoch 94/100\n",
      "160/160 [==============================] - 0s 642us/step - loss: 80.3152 - binary_accuracy: 0.6885\n",
      "Epoch 95/100\n",
      "160/160 [==============================] - 0s 642us/step - loss: 80.3293 - binary_accuracy: 0.6885\n",
      "Epoch 96/100\n",
      "160/160 [==============================] - 0s 789us/step - loss: 80.2933 - binary_accuracy: 0.6888\n",
      "Epoch 97/100\n",
      "160/160 [==============================] - 0s 643us/step - loss: 80.3203 - binary_accuracy: 0.6888\n",
      "Epoch 98/100\n",
      "160/160 [==============================] - 0s 744us/step - loss: 80.3149 - binary_accuracy: 0.6887\n",
      "Epoch 99/100\n",
      "160/160 [==============================] - 0s 657us/step - loss: 80.2720 - binary_accuracy: 0.6888\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 0s 648us/step - loss: 80.2761 - binary_accuracy: 0.6888\n",
      "250/250 [==============================] - 0s 448us/step\n",
      "250/250 [==============================] - 0s 357us/step\n"
     ]
    }
   ],
   "source": [
    "vae_q = VariationalAutoEncoder(NUM_STATS, INTERMEDIATE_DIM, NUM_SKILLS, N_DECODERS)\n",
    "\n",
    "# Optimizer\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate=0.005, amsgrad=True)\n",
    "opt = tf.keras.optimizers.legacy.SGD()\n",
    "\n",
    "vae_q.compile(optimizer=opt, loss=vae_q.vae_loss, metrics=['binary_accuracy'])\n",
    "\n",
    "y = pd.DataFrame(data.values.flatten()) # Item responde values\n",
    "\n",
    "# Para executar o y imputado tem que colocar o range com valor acima de 1\n",
    "for i in range(2):\n",
    "    print(\"\\n %d Iteration #################################################################################### \\n\" % (i+1))\n",
    "    #data_train = pd.DataFrame(y.values.reshape(num_stats, N))\n",
    "    dtrain = tf.cast(data, tf.float32)\n",
    "\n",
    "    history = vae_q.fit(dtrain,\n",
    "                        dtrain,\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True)\n",
    "    # validation_split=0.2\n",
    "    #ba = 0\n",
    "    #for value in history.history['binary_accuracy']:\n",
    "    #    ba += value\n",
    "    #print(\"Binary Accuracy: %.4f\" % (ba / 25))\n",
    "\n",
    "    encoder = vae_q.get_encoder()\n",
    "    decoder = vae_q.get_decoder()\n",
    "\n",
    "    weights = vae_q._get_weights()\n",
    "\n",
    "    discr = weights[0].numpy()\n",
    "    #diff = pd.DataFrame(weights[3].numpy())\n",
    "    negative_diff_20 = pd.DataFrame(np.negative(weights[1].numpy()))\n",
    "\n",
    "    # Get latent trait predictions\n",
    "    thetas_hat20, log_var_thetas_hat20, z_pred20 = encoder.predict(data)\n",
    "    \n",
    "\n",
    "    # Get mean.\n",
    "    if N_DECODERS > 1:\n",
    "        dec_pred = [decoder.predict(z_pred20[i]) for i in range(5)]\n",
    "        main = []\n",
    "        \n",
    "        for i in range(NUM_PERSONS):\n",
    "            lst = []\n",
    "            for j in range(NUM_STATS):\n",
    "                mu = [dec_pred[k][i][j] for k in range(N_DECODERS)]\n",
    "                lst.append(np.mean(mu))\n",
    "                mu.clear()\n",
    "            main.append(lst)\n",
    "    else:\n",
    "        dec_pred = decoder.predict(z_pred20)\n",
    "        main = dec_pred\n",
    "    \n",
    "\n",
    "    # 1 if mean equal or greater than 0.5 and 0 otherwise\n",
    "    for i in range(NUM_PERSONS):\n",
    "        for j in range(NUM_STATS):\n",
    "            if main[i][j] >= 0.5:\n",
    "                main[i][j] = 1 \n",
    "            else:\n",
    "                main[i][j] = 0\n",
    "\n",
    "    main = np.array(main)\n",
    "    Y_means = main.T\n",
    "\n",
    "    # Vectorize in Y_means\n",
    "    Y_means = []\n",
    "    for line in main:\n",
    "        for l in line:\n",
    "            Y_means.append(l)\n",
    "\n",
    "\n",
    "    # Imputation\n",
    "    Y_imputated = []\n",
    "\n",
    "    for i in range(NUM_PERSONS * NUM_STATS):\n",
    "        if y.values[i] == -1.0:\n",
    "            Y_imputated.append(Y_means[i])\n",
    "        else:\n",
    "            Y_imputated.append(y.values[i].item())\n",
    "\n",
    "    y = pd.DataFrame(Y_imputated)\n",
    "\n",
    "    vae_q = VariationalAutoEncoder(NUM_STATS, INTERMEDIATE_DIM, NUM_SKILLS, N_DECODERS)\n",
    "\n",
    "    vae_q.compile(optimizer=opt, loss=vae_q.vae_loss, metrics=['binary_accuracy'])\n",
    "\n",
    "    # Total score on the test -------\n",
    "    score = np.apply_over_axes(np.sum, dtrain, 1)\n",
    "\n",
    "    #### Vectoring the matrices Thetas_hat ans discr ####\n",
    "    theta_hat = np.transpose(thetas_hat20).flatten()\n",
    "    #step_theta_hat = np.transpose(step_thetas_hat.numpy()).flatten()\n",
    "\n",
    "    log_var_theta_hat = np.transpose(log_var_thetas_hat20).flatten()\n",
    "    #step_log_var_theta_hat = np.transpose(step_log_var_thetas_hat.numpy()).flatten()\n",
    "\n",
    "    discr_hat_20 = remove_zeros(discr)\n",
    "\n",
    "    # Correlation\n",
    "    #reshaped = theta_hat.reshape((theta_hat.shape[0], 1))\n",
    "    #df_theta_hat = pd.DataFrame(reshaped)\n",
    "    #df_thetas_r_vae = pd.DataFrame(thetas_r_vae.values.flatten())\n",
    "\n",
    "    #print(\"\\n CORRELAÇÃO THETAS R VAE: %.4f \\n\" % df_theta_hat.corrwith(df_thetas_r_vae, method='pearson'))\n",
    "    #print(\"\\n CORRELAÇÃO THETAS PYTHON VAE: %.4f \\n\" % df_theta_hat.corrwith(thetas_python_vae['Thetas Estimation'], method='pearson'))\n",
    "    \n",
    "                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.to_csv('output/y_claudia_10_missing_180x21.csv', sep=';', header=None)\n",
    "np.savetxt('output/thetas_claudia_10_missing_180x21.csv', theta_hat, delimiter=';')\n",
    "np.savetxt('output/discr_claudia_10_missing_180x21.csv', discr_hat_20, delimiter=';')\n",
    "np.savetxt('output/diff_claudia_10_missing_180x21.csv', negative_diff_20, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "924285c36e9c91e77dd67ccd6618b5b801447ab41be1cbc841fddd0b5e954220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
